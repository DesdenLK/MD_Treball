{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as ds\n",
    "import sklearn.model_selection as cv\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, cross_validate\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier,\n",
    "    BaggingClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    RandomForestClassifier,\n",
    "    VotingClassifier,    \n",
    "    GradientBoostingClassifier\n",
    ")\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)  # To ignore warnings due to deprecated functions\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 55494 samples and 25 features.\n",
      "         gender       age    height    weight     waist    sightL    sightR  \\\n",
      "0     -1.319926 -0.344157 -1.050078 -0.457567 -0.079491  0.618616  0.021820   \n",
      "1     -1.319926 -0.344157 -0.506364 -0.457567 -0.111843 -0.607480 -1.210286   \n",
      "2      0.757618  0.900109  0.581062 -0.457567 -0.219682 -0.607480 -0.594233   \n",
      "3      0.757618 -0.344157  0.037349  0.322412  0.643036  1.538188  1.561952   \n",
      "4     -1.319926 -0.344157 -1.050078 -0.457567  0.427356  0.005568  0.021820   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "55489 -1.319926 -0.344157  0.581062 -0.067577 -0.758881 -0.300956 -0.286207   \n",
      "55490 -1.319926  0.070598 -0.506364 -1.237546 -1.298080  0.618616  0.637872   \n",
      "55491 -1.319926  0.900109 -0.506364 -1.237546 -1.459840  0.005568  0.637872   \n",
      "55492  0.757618  1.314865  0.037349 -0.457567 -0.435362 -0.607480  0.021820   \n",
      "55493  0.757618  0.900109 -0.506364 -0.067577  0.319516 -0.300956 -0.902260   \n",
      "\n",
      "         hearL    hearR  systolicP  ...       hdl       ldl  hemoglobin  \\\n",
      "0     -0.16117 -0.16281  -0.547109  ...  1.065161  0.343301   -1.100271   \n",
      "1     -0.16117 -0.16281  -0.181415  ... -1.037549  0.373183   -1.228038   \n",
      "2     -0.16117 -0.16281   1.208219  ... -0.155768  1.090357    0.752354   \n",
      "3     -0.16117 -0.16281  -1.571050  ... -0.834061  3.331526    0.049634   \n",
      "4     -0.16117 -0.16281  -0.108277  ...  0.319038 -0.224461   -1.355805   \n",
      "...        ...      ...        ...  ...       ...       ...         ...   \n",
      "55489 -0.16117 -0.16281  -0.839663  ...  1.200819  0.104243   -1.483572   \n",
      "55490 -0.16117 -0.16281  -1.497911  ...  1.065161 -1.061164   -0.397551   \n",
      "55491 -0.16117 -0.16281  -0.327693  ...  1.472137 -1.539280   -1.419689   \n",
      "55492 -0.16117 -0.16281   0.842526  ... -0.630573  0.940946   -0.142017   \n",
      "55493 -0.16117 -0.16281   0.184278  ... -1.580184  1.060475    0.241285   \n",
      "\n",
      "       uProtein  sCreatinine       ast       alt       gtp    caries    tartar  \n",
      "0     -0.215324    -0.851209 -0.422099 -0.259350 -0.256956 -0.521015  0.894199  \n",
      "1     -0.215324    -1.310208 -0.215740 -0.259350 -0.435846 -0.521015  0.894199  \n",
      "2     -0.215324     0.525789 -0.267330 -0.356156 -0.356339 -0.521015 -1.118320  \n",
      "3     -0.215324     0.525789 -0.370509 -0.033469 -0.435846 -0.521015  0.894199  \n",
      "4     -0.215324    -1.310208 -0.525278 -0.420694 -0.356339 -0.521015 -1.118320  \n",
      "...         ...          ...       ...       ...       ...       ...       ...  \n",
      "55489 -0.215324    -1.310208 -0.628457 -0.646575 -0.594858  1.919330  0.894199  \n",
      "55490 -0.215324     0.066790 -0.318919 -0.485231 -0.515352 -0.521015  0.894199  \n",
      "55491 -0.215324    -1.769207 -0.473688 -0.517500 -0.555105 -0.521015 -1.118320  \n",
      "55492 -0.215324    -0.851209 -0.318919 -0.259350 -0.435846 -0.521015 -1.118320  \n",
      "55493 -0.215324    -0.392210 -0.009382  0.063337  0.021316 -0.521015  0.894199  \n",
      "\n",
      "[55494 rows x 24 columns]\n",
      "0        0.0\n",
      "1        0.0\n",
      "2        1.0\n",
      "3        0.0\n",
      "4        0.0\n",
      "        ... \n",
      "55489    0.0\n",
      "55490    0.0\n",
      "55491    0.0\n",
      "55492    0.0\n",
      "55493    1.0\n",
      "Name: smoking, Length: 55494, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#LOAD DE DATASET\n",
    "smoking = pd.read_csv(\n",
    "    \"./Dataset/Standardized_Smoking.csv\",  # We can select a local file or pass an url\n",
    "    sep = ',', \n",
    ")\n",
    "X = smoking.iloc[:, :-1]\n",
    "y = smoking.loc[:, \"smoking\"]\n",
    "undersampler = RandomUnderSampler(sampling_strategy=\"majority\", random_state=1)\n",
    "print(f\"We have {smoking.shape[0]} samples and {smoking.shape[1]} features.\")  # Shape returns (nº samples, nº features)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to measure execution's time. It will be use as a 'decorator'\n",
    "# The idea behind this is just to use it to estimate the time it will take for the\n",
    "# full code to run if you want to know before training with the complete dataset\n",
    "# using a subset of size known in relationship with the full dataset.\n",
    "def compute_executions_time(function):\n",
    "    def wrapper():\n",
    "        start_time = time.time()  # init measuring time\n",
    "        function()  # execute function\n",
    "        print(f\"\\n{time.time() - start_time} seconds\")  # print execution time\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm:=Naive Bayes\n",
      "Mean F1 Score: 0.679\n",
      "Mean Recall: 0.850\n",
      "Mean Precision: 0.565\n",
      "------------------------------\n",
      "Algorithm:=Knn\n",
      "Mean F1 Score: 0.765\n",
      "Mean Recall: 0.915\n",
      "Mean Precision: 0.658\n",
      "------------------------------\n",
      "Algorithm:=Dec. Tree\n",
      "Mean F1 Score: 0.725\n",
      "Mean Recall: 0.796\n",
      "Mean Precision: 0.665\n",
      "------------------------------\n",
      "Algorithm:= Majority Voting HARD\n",
      "Mean F1 Score: 0.765\n",
      "Mean Recall: 0.922\n",
      "Mean Precision: 0.655\n",
      "------------------------------\n",
      "Algorithm:= Majority Voting SOFT\n",
      "Mean F1 Score: 0.753\n",
      "Mean Recall: 0.832\n",
      "Mean Precision: 0.688\n",
      "------------------------------\n",
      "\n",
      "17.901648998260498 seconds\n"
     ]
    }
   ],
   "source": [
    "@compute_executions_time\n",
    "def execute_voting_scheme_different_estimators_grid_search_and_cv(cv=10):\n",
    "    naive_bayes = GaussianNB()\n",
    "\n",
    "    clf2 = KNeighborsClassifier(\n",
    "        n_neighbors = 27,\n",
    "        weights = 'distance'\n",
    "    )\n",
    "    clf3 = DecisionTreeClassifier(\n",
    "    criterion=\"entropy\",\n",
    "    min_samples_split=2,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_samples_leaf=1,\n",
    "    max_depth=42\n",
    ")\n",
    "    \n",
    "    for clf, label in zip([naive_bayes, clf2, clf3], [\"Naive Bayes\",\"Knn\", \"Dec. Tree\", ]):\n",
    "        pipeline = Pipeline(steps=[('undersampler', undersampler), ('model', clf)])\n",
    "        scores = cross_validate(\n",
    "            pipeline, \n",
    "            X, \n",
    "            y, \n",
    "            cv = cv, \n",
    "            scoring=[\"f1\", \"recall\", \"precision\"]\n",
    "        )\n",
    "        mean_f1 = scores['test_f1'].mean()\n",
    "        mean_recall = scores['test_recall'].mean()\n",
    "        mean_precision = scores['test_precision'].mean()\n",
    "        \n",
    "        print(f\"Algorithm:={label}\")\n",
    "        print(f\"Mean F1 Score: {mean_f1:.3f}\")\n",
    "        print(f\"Mean Recall: {mean_recall:.3f}\")\n",
    "        print(f\"Mean Precision: {mean_precision:.3f}\")\n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    voting_classifier = VotingClassifier(\n",
    "        estimators=[\n",
    "            (\"nb\", naive_bayes),\n",
    "            (\"knn\", clf2),\n",
    "            (\"dt\", clf3)\n",
    "        ],\n",
    "        voting=\"hard\"\n",
    "    )\n",
    "    pipeline = Pipeline(steps=[('undersampler', undersampler), ('model', voting_classifier)])\n",
    "    scores = cross_validate(\n",
    "        pipeline,\n",
    "        X,\n",
    "        y,\n",
    "        cv = cv,\n",
    "        scoring=[\"f1\", \"recall\", \"precision\"]\n",
    "    )\n",
    "    mean_f1 = scores['test_f1'].mean()\n",
    "    mean_recall = scores['test_recall'].mean()\n",
    "    mean_precision = scores['test_precision'].mean()\n",
    "    \n",
    "    print(f\"Algorithm:= Majority Voting HARD\")\n",
    "    print(f\"Mean F1 Score: {mean_f1:.3f}\")\n",
    "    print(f\"Mean Recall: {mean_recall:.3f}\")\n",
    "    print(f\"Mean Precision: {mean_precision:.3f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    voting_classifier2 = VotingClassifier(\n",
    "        estimators=[\n",
    "            (\"nb\", naive_bayes),\n",
    "            (\"knn\", clf2),\n",
    "            (\"dt\", clf3)\n",
    "        ],\n",
    "        voting=\"soft\",\n",
    "        weights=[1,2,2]\n",
    "    )\n",
    "    pipeline = Pipeline(steps=[('undersampler', undersampler), ('model', voting_classifier2)])\n",
    "    scores = cross_validate(\n",
    "        pipeline,\n",
    "        X,\n",
    "        y,\n",
    "        cv = cv,\n",
    "        scoring=[\"f1\", \"recall\", \"precision\"]\n",
    "    )\n",
    "    mean_f1 = scores['test_f1'].mean()\n",
    "    mean_recall = scores['test_recall'].mean()\n",
    "    mean_precision = scores['test_precision'].mean()\n",
    "    \n",
    "    print(f\"Algorithm:= Majority Voting SOFT\")\n",
    "    print(f\"Mean F1 Score: {mean_f1:.3f}\")\n",
    "    print(f\"Mean Recall: {mean_recall:.3f}\")\n",
    "    print(f\"Mean Precision: {mean_precision:.3f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "\n",
    "execute_voting_scheme_different_estimators_grid_search_and_cv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WITH KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAX FEATURES NOT FIXED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=1\n",
      "Mean F1 Score: 0.732\n",
      "Mean Recall: 0.869\n",
      "Mean Precision: 0.632\n",
      "------------------------------\n",
      "n_estimators=2\n",
      "Mean F1 Score: 0.751\n",
      "Mean Recall: 0.894\n",
      "Mean Precision: 0.648\n",
      "------------------------------\n",
      "n_estimators=5\n",
      "Mean F1 Score: 0.760\n",
      "Mean Recall: 0.911\n",
      "Mean Precision: 0.653\n",
      "------------------------------\n",
      "n_estimators=10\n",
      "Mean F1 Score: 0.763\n",
      "Mean Recall: 0.914\n",
      "Mean Precision: 0.655\n",
      "------------------------------\n",
      "n_estimators=20\n",
      "Mean F1 Score: 0.764\n",
      "Mean Recall: 0.916\n",
      "Mean Precision: 0.656\n",
      "------------------------------\n",
      "n_estimators=50\n",
      "Mean F1 Score: 0.765\n",
      "Mean Recall: 0.917\n",
      "Mean Precision: 0.657\n",
      "------------------------------\n",
      "n_estimators=100\n",
      "Mean F1 Score: 0.765\n",
      "Mean Recall: 0.917\n",
      "Mean Precision: 0.657\n",
      "------------------------------\n",
      "n_estimators=200\n",
      "Mean F1 Score: 0.765\n",
      "Mean Recall: 0.918\n",
      "Mean Precision: 0.657\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(\n",
    "    criterion=\"entropy\",\n",
    "    min_samples_split=2,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_samples_leaf=1,\n",
    "    max_depth=42\n",
    ")\n",
    "gb = GaussianNB()\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=27,\n",
    "    weights='distance'\n",
    ")\n",
    "for nest in [1, 2, 5, 10, 20, 50, 100, 200]:\n",
    "    model = BaggingClassifier(\n",
    "            estimator = knn,\n",
    "            n_estimators = nest,\n",
    "            #max_features=0.35\n",
    "        )\n",
    "    pipeline = Pipeline(steps=[('undersampler', undersampler), ('model', model)])\n",
    "    \n",
    "    scores = cross_validate(\n",
    "        pipeline, \n",
    "        X, \n",
    "        y, \n",
    "        cv = 10, \n",
    "        scoring=[\"f1\", \"recall\", \"precision\"]\n",
    "    )\n",
    "    mean_f1 = scores['test_f1'].mean()\n",
    "    mean_recall = scores['test_recall'].mean()\n",
    "    mean_precision = scores['test_precision'].mean()\n",
    "    \n",
    "    print(f\"n_estimators={nest}\")\n",
    "    print(f\"Mean F1 Score: {mean_f1:.3f}\")\n",
    "    print(f\"Mean Recall: {mean_recall:.3f}\")\n",
    "    print(f\"Mean Precision: {mean_precision:.3f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAX FEATUERS FIXED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=1\n",
      "Mean F1 Score: 0.689\n",
      "Mean Recall: 0.813\n",
      "Mean Precision: 0.599\n",
      "------------------------------\n",
      "n_estimators=2\n",
      "Mean F1 Score: 0.735\n",
      "Mean Recall: 0.870\n",
      "Mean Precision: 0.637\n",
      "------------------------------\n",
      "n_estimators=5\n",
      "Mean F1 Score: 0.757\n",
      "Mean Recall: 0.915\n",
      "Mean Precision: 0.647\n",
      "------------------------------\n",
      "n_estimators=10\n",
      "Mean F1 Score: 0.763\n",
      "Mean Recall: 0.929\n",
      "Mean Precision: 0.648\n",
      "------------------------------\n",
      "n_estimators=20\n",
      "Mean F1 Score: 0.766\n",
      "Mean Recall: 0.945\n",
      "Mean Precision: 0.646\n",
      "------------------------------\n",
      "n_estimators=50\n",
      "Mean F1 Score: 0.765\n",
      "Mean Recall: 0.946\n",
      "Mean Precision: 0.643\n",
      "------------------------------\n",
      "n_estimators=100\n",
      "Mean F1 Score: 0.766\n",
      "Mean Recall: 0.949\n",
      "Mean Precision: 0.643\n",
      "------------------------------\n",
      "n_estimators=200\n",
      "Mean F1 Score: 0.765\n",
      "Mean Recall: 0.946\n",
      "Mean Precision: 0.643\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(\n",
    "    criterion=\"entropy\",\n",
    "    min_samples_split=2,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_samples_leaf=1,\n",
    "    max_depth=42\n",
    ")\n",
    "gb = GaussianNB()\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=27,\n",
    "    weights='distance'\n",
    ")\n",
    "for nest in [1, 2, 5, 10, 20, 50, 100, 200]:\n",
    "    model = BaggingClassifier(\n",
    "            estimator = knn,\n",
    "            n_estimators = nest,\n",
    "            max_features=0.35\n",
    "        )\n",
    "    pipeline = Pipeline(steps=[('undersampler', undersampler), ('model', model)])\n",
    "    \n",
    "    scores = cross_validate(\n",
    "        pipeline, \n",
    "        X, \n",
    "        y, \n",
    "        cv = 10, \n",
    "        scoring=[\"f1\", \"recall\", \"precision\"]\n",
    "    )\n",
    "    mean_f1 = scores['test_f1'].mean()\n",
    "    mean_recall = scores['test_recall'].mean()\n",
    "    mean_precision = scores['test_precision'].mean()\n",
    "    \n",
    "    print(f\"n_estimators={nest}\")\n",
    "    print(f\"Mean F1 Score: {mean_f1:.3f}\")\n",
    "    print(f\"Mean Recall: {mean_recall:.3f}\")\n",
    "    print(f\"Mean Precision: {mean_precision:.3f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WITH DECISION TREES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAX FEATURES NOT FIXED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=1\n",
      "Mean F1 Score: 0.683\n",
      "Mean Recall: 0.753\n",
      "Mean Precision: 0.625\n",
      "------------------------------\n",
      "n_estimators=2\n",
      "Mean F1 Score: 0.645\n",
      "Mean Recall: 0.594\n",
      "Mean Precision: 0.708\n",
      "------------------------------\n",
      "n_estimators=5\n",
      "Mean F1 Score: 0.748\n",
      "Mean Recall: 0.843\n",
      "Mean Precision: 0.672\n",
      "------------------------------\n",
      "n_estimators=10\n",
      "Mean F1 Score: 0.761\n",
      "Mean Recall: 0.835\n",
      "Mean Precision: 0.699\n",
      "------------------------------\n",
      "n_estimators=20\n",
      "Mean F1 Score: 0.774\n",
      "Mean Recall: 0.877\n",
      "Mean Precision: 0.692\n",
      "------------------------------\n",
      "n_estimators=50\n",
      "Mean F1 Score: 0.779\n",
      "Mean Recall: 0.903\n",
      "Mean Precision: 0.685\n",
      "------------------------------\n",
      "n_estimators=100\n",
      "Mean F1 Score: 0.780\n",
      "Mean Recall: 0.911\n",
      "Mean Precision: 0.682\n",
      "------------------------------\n",
      "n_estimators=200\n",
      "Mean F1 Score: 0.780\n",
      "Mean Recall: 0.915\n",
      "Mean Precision: 0.680\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(\n",
    "    criterion=\"entropy\",\n",
    "    min_samples_split=2,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_samples_leaf=1,\n",
    "    max_depth=42\n",
    ")\n",
    "gb = GaussianNB()\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=27,\n",
    "    weights='distance'\n",
    ")\n",
    "for nest in [1, 2, 5, 10, 20, 50, 100, 200]:\n",
    "    model = BaggingClassifier(\n",
    "            estimator = dt,\n",
    "            n_estimators = nest,\n",
    "            #max_features=0.35\n",
    "        )\n",
    "    pipeline = Pipeline(steps=[('undersampler', undersampler), ('model', model)])\n",
    "    \n",
    "    scores = cross_validate(\n",
    "        pipeline, \n",
    "        X, \n",
    "        y, \n",
    "        cv = 10, \n",
    "        scoring=[\"f1\", \"recall\", \"precision\"]\n",
    "    )\n",
    "    mean_f1 = scores['test_f1'].mean()\n",
    "    mean_recall = scores['test_recall'].mean()\n",
    "    mean_precision = scores['test_precision'].mean()\n",
    "    \n",
    "    print(f\"n_estimators={nest}\")\n",
    "    print(f\"Mean F1 Score: {mean_f1:.3f}\")\n",
    "    print(f\"Mean Recall: {mean_recall:.3f}\")\n",
    "    print(f\"Mean Precision: {mean_precision:.3f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAX FEATURES FIXED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=1\n",
      "Mean F1 Score: 0.643\n",
      "Mean Recall: 0.721\n",
      "Mean Precision: 0.581\n",
      "------------------------------\n",
      "n_estimators=2\n",
      "Mean F1 Score: 0.596\n",
      "Mean Recall: 0.536\n",
      "Mean Precision: 0.675\n",
      "------------------------------\n",
      "n_estimators=5\n",
      "Mean F1 Score: 0.722\n",
      "Mean Recall: 0.813\n",
      "Mean Precision: 0.649\n",
      "------------------------------\n",
      "n_estimators=10\n",
      "Mean F1 Score: 0.736\n",
      "Mean Recall: 0.797\n",
      "Mean Precision: 0.683\n",
      "------------------------------\n",
      "n_estimators=20\n",
      "Mean F1 Score: 0.757\n",
      "Mean Recall: 0.854\n",
      "Mean Precision: 0.680\n",
      "------------------------------\n",
      "n_estimators=50\n",
      "Mean F1 Score: 0.765\n",
      "Mean Recall: 0.898\n",
      "Mean Precision: 0.667\n",
      "------------------------------\n",
      "n_estimators=100\n",
      "Mean F1 Score: 0.769\n",
      "Mean Recall: 0.921\n",
      "Mean Precision: 0.661\n",
      "------------------------------\n",
      "n_estimators=200\n",
      "Mean F1 Score: 0.769\n",
      "Mean Recall: 0.929\n",
      "Mean Precision: 0.657\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(\n",
    "    criterion=\"entropy\",\n",
    "    min_samples_split=2,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_samples_leaf=1,\n",
    "    max_depth=42\n",
    ")\n",
    "gb = GaussianNB()\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=27,\n",
    "    weights='distance'\n",
    ")\n",
    "for nest in [1, 2, 5, 10, 20, 50, 100, 200]:\n",
    "    model = BaggingClassifier(\n",
    "            estimator = dt,\n",
    "            n_estimators = nest,\n",
    "            max_features=0.35\n",
    "        )\n",
    "    pipeline = Pipeline(steps=[('undersampler', undersampler), ('model', model)])\n",
    "    \n",
    "    scores = cross_validate(\n",
    "        pipeline, \n",
    "        X, \n",
    "        y, \n",
    "        cv = 10, \n",
    "        scoring=[\"f1\", \"recall\", \"precision\"]\n",
    "    )\n",
    "    mean_f1 = scores['test_f1'].mean()\n",
    "    mean_recall = scores['test_recall'].mean()\n",
    "    mean_precision = scores['test_precision'].mean()\n",
    "    \n",
    "    print(f\"n_estimators={nest}\")\n",
    "    print(f\"Mean F1 Score: {mean_f1:.3f}\")\n",
    "    print(f\"Mean Recall: {mean_recall:.3f}\")\n",
    "    print(f\"Mean Precision: {mean_precision:.3f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WITH NAIVE BAYES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAX FEATURES NOT FIXED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=1\n",
      "Mean F1 Score: 0.679\n",
      "Mean Recall: 0.851\n",
      "Mean Precision: 0.564\n",
      "------------------------------\n",
      "n_estimators=2\n",
      "Mean F1 Score: 0.676\n",
      "Mean Recall: 0.840\n",
      "Mean Precision: 0.566\n",
      "------------------------------\n",
      "n_estimators=5\n",
      "Mean F1 Score: 0.679\n",
      "Mean Recall: 0.851\n",
      "Mean Precision: 0.565\n",
      "------------------------------\n",
      "n_estimators=10\n",
      "Mean F1 Score: 0.679\n",
      "Mean Recall: 0.849\n",
      "Mean Precision: 0.565\n",
      "------------------------------\n",
      "n_estimators=20\n",
      "Mean F1 Score: 0.678\n",
      "Mean Recall: 0.847\n",
      "Mean Precision: 0.565\n",
      "------------------------------\n",
      "n_estimators=50\n",
      "Mean F1 Score: 0.679\n",
      "Mean Recall: 0.849\n",
      "Mean Precision: 0.565\n",
      "------------------------------\n",
      "n_estimators=100\n",
      "Mean F1 Score: 0.679\n",
      "Mean Recall: 0.849\n",
      "Mean Precision: 0.565\n",
      "------------------------------\n",
      "n_estimators=200\n",
      "Mean F1 Score: 0.679\n",
      "Mean Recall: 0.849\n",
      "Mean Precision: 0.566\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(\n",
    "    criterion=\"entropy\",\n",
    "    min_samples_split=2,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_samples_leaf=1,\n",
    "    max_depth=42\n",
    ")\n",
    "gb = GaussianNB()\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=27,\n",
    "    weights='distance'\n",
    ")\n",
    "for nest in [1, 2, 5, 10, 20, 50, 100, 200]:\n",
    "    model = BaggingClassifier(\n",
    "            estimator = gb,\n",
    "            n_estimators = nest,\n",
    "            #max_features=0.35\n",
    "        )\n",
    "    pipeline = Pipeline(steps=[('undersampler', undersampler), ('model', model)])\n",
    "    \n",
    "    scores = cross_validate(\n",
    "        pipeline, \n",
    "        X, \n",
    "        y, \n",
    "        cv = 10, \n",
    "        scoring=[\"f1\", \"recall\", \"precision\"]\n",
    "    )\n",
    "    mean_f1 = scores['test_f1'].mean()\n",
    "    mean_recall = scores['test_recall'].mean()\n",
    "    mean_precision = scores['test_precision'].mean()\n",
    "    \n",
    "    print(f\"n_estimators={nest}\")\n",
    "    print(f\"Mean F1 Score: {mean_f1:.3f}\")\n",
    "    print(f\"Mean Recall: {mean_recall:.3f}\")\n",
    "    print(f\"Mean Precision: {mean_precision:.3f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAX FEATURES FIXED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=1\n",
      "Mean F1 Score: 0.614\n",
      "Mean Recall: 0.730\n",
      "Mean Precision: 0.563\n",
      "------------------------------\n",
      "n_estimators=2\n",
      "Mean F1 Score: 0.667\n",
      "Mean Recall: 0.824\n",
      "Mean Precision: 0.563\n",
      "------------------------------\n",
      "n_estimators=5\n",
      "Mean F1 Score: 0.653\n",
      "Mean Recall: 0.780\n",
      "Mean Precision: 0.565\n",
      "------------------------------\n",
      "n_estimators=10\n",
      "Mean F1 Score: 0.671\n",
      "Mean Recall: 0.829\n",
      "Mean Precision: 0.565\n",
      "------------------------------\n",
      "n_estimators=20\n",
      "Mean F1 Score: 0.679\n",
      "Mean Recall: 0.854\n",
      "Mean Precision: 0.565\n",
      "------------------------------\n",
      "n_estimators=50\n",
      "Mean F1 Score: 0.679\n",
      "Mean Recall: 0.850\n",
      "Mean Precision: 0.566\n",
      "------------------------------\n",
      "n_estimators=100\n",
      "Mean F1 Score: 0.678\n",
      "Mean Recall: 0.846\n",
      "Mean Precision: 0.566\n",
      "------------------------------\n",
      "n_estimators=200\n",
      "Mean F1 Score: 0.678\n",
      "Mean Recall: 0.844\n",
      "Mean Precision: 0.566\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(\n",
    "    criterion=\"entropy\",\n",
    "    min_samples_split=2,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_samples_leaf=1,\n",
    "    max_depth=42\n",
    ")\n",
    "gb = GaussianNB()\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=27,\n",
    "    weights='distance'\n",
    ")\n",
    "for nest in [1, 2, 5, 10, 20, 50, 100, 200]:\n",
    "    model = BaggingClassifier(\n",
    "            estimator = gb,\n",
    "            n_estimators = nest,\n",
    "            max_features=0.35\n",
    "        )\n",
    "    pipeline = Pipeline(steps=[('undersampler', undersampler), ('model', model)])\n",
    "    \n",
    "    scores = cross_validate(\n",
    "        pipeline, \n",
    "        X, \n",
    "        y, \n",
    "        cv = 10, \n",
    "        scoring=[\"f1\", \"recall\", \"precision\"]\n",
    "    )\n",
    "    mean_f1 = scores['test_f1'].mean()\n",
    "    mean_recall = scores['test_recall'].mean()\n",
    "    mean_precision = scores['test_precision'].mean()\n",
    "    \n",
    "    print(f\"n_estimators={nest}\")\n",
    "    print(f\"Mean F1 Score: {mean_f1:.3f}\")\n",
    "    print(f\"Mean Recall: {mean_recall:.3f}\")\n",
    "    print(f\"Mean Precision: {mean_precision:.3f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAX FEATURES NOT FIXED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=1\n",
      "Mean F1 Score: 0.671\n",
      "Mean Recall: 0.743\n",
      "Mean Precision: 0.613\n",
      "------------------------------\n",
      "n_estimators=2\n",
      "Mean F1 Score: 0.639\n",
      "Mean Recall: 0.584\n",
      "Mean Precision: 0.706\n",
      "------------------------------\n",
      "n_estimators=5\n",
      "Mean F1 Score: 0.745\n",
      "Mean Recall: 0.840\n",
      "Mean Precision: 0.669\n",
      "------------------------------\n",
      "n_estimators=10\n",
      "Mean F1 Score: 0.760\n",
      "Mean Recall: 0.834\n",
      "Mean Precision: 0.697\n",
      "------------------------------\n",
      "n_estimators=20\n",
      "Mean F1 Score: 0.772\n",
      "Mean Recall: 0.881\n",
      "Mean Precision: 0.688\n",
      "------------------------------\n",
      "n_estimators=50\n",
      "Mean F1 Score: 0.778\n",
      "Mean Recall: 0.912\n",
      "Mean Precision: 0.679\n",
      "------------------------------\n",
      "n_estimators=100\n",
      "Mean F1 Score: 0.779\n",
      "Mean Recall: 0.923\n",
      "Mean Precision: 0.675\n",
      "------------------------------\n",
      "n_estimators=200\n",
      "Mean F1 Score: 0.779\n",
      "Mean Recall: 0.927\n",
      "Mean Precision: 0.673\n",
      "------------------------------\n",
      "\n",
      "181.67126631736755 seconds\n"
     ]
    }
   ],
   "source": [
    "@compute_executions_time\n",
    "def execute_random_forest_with_different_estimators_and_cv(cv=10):\n",
    "    for n_trees in [1, 2, 5, 10, 20, 50, 100, 200]:\n",
    "        random_forest_classifier = RandomForestClassifier(\n",
    "            n_estimators = n_trees,\n",
    "            #max_features=0.35\n",
    "        )\n",
    "        pipeline = Pipeline(steps=[('undersampler', undersampler), ('model', random_forest_classifier)])\n",
    "        scores = cross_validate(\n",
    "            estimator = pipeline,\n",
    "            X = X,\n",
    "            y = y,\n",
    "            cv=cv,\n",
    "            scoring=[\"f1\", \"recall\", \"precision\"]\n",
    "        )\n",
    "        mean_f1 = scores['test_f1'].mean()\n",
    "        mean_recall = scores['test_recall'].mean()\n",
    "        mean_precision = scores['test_precision'].mean()\n",
    "        \n",
    "        print(f\"n_estimators={n_trees}\")\n",
    "        print(f\"Mean F1 Score: {mean_f1:.3f}\")\n",
    "        print(f\"Mean Recall: {mean_recall:.3f}\")\n",
    "        print(f\"Mean Precision: {mean_precision:.3f}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "execute_random_forest_with_different_estimators_and_cv();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAX FEATUERS FIXED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=1\n",
      "Mean F1 Score: 0.679\n",
      "Mean Recall: 0.751\n",
      "Mean Precision: 0.620\n",
      "------------------------------\n",
      "n_estimators=2\n",
      "Mean F1 Score: 0.644\n",
      "Mean Recall: 0.591\n",
      "Mean Precision: 0.709\n",
      "------------------------------\n",
      "n_estimators=5\n",
      "Mean F1 Score: 0.747\n",
      "Mean Recall: 0.844\n",
      "Mean Precision: 0.670\n",
      "------------------------------\n",
      "n_estimators=10\n",
      "Mean F1 Score: 0.760\n",
      "Mean Recall: 0.831\n",
      "Mean Precision: 0.700\n",
      "------------------------------\n",
      "n_estimators=20\n",
      "Mean F1 Score: 0.774\n",
      "Mean Recall: 0.878\n",
      "Mean Precision: 0.692\n",
      "------------------------------\n",
      "n_estimators=50\n",
      "Mean F1 Score: 0.776\n",
      "Mean Recall: 0.903\n",
      "Mean Precision: 0.681\n",
      "------------------------------\n",
      "n_estimators=100\n",
      "Mean F1 Score: 0.778\n",
      "Mean Recall: 0.915\n",
      "Mean Precision: 0.678\n",
      "------------------------------\n",
      "n_estimators=200\n",
      "Mean F1 Score: 0.780\n",
      "Mean Recall: 0.921\n",
      "Mean Precision: 0.677\n",
      "------------------------------\n",
      "\n",
      "319.6738233566284 seconds\n"
     ]
    }
   ],
   "source": [
    "@compute_executions_time\n",
    "def execute_random_forest_with_different_estimators_and_cv(cv=10):\n",
    "    for n_trees in [1, 2, 5, 10, 20, 50, 100, 200]:\n",
    "        random_forest_classifier = RandomForestClassifier(\n",
    "            n_estimators = n_trees,\n",
    "            max_features=0.35\n",
    "        )\n",
    "        pipeline = Pipeline(steps=[('undersampler', undersampler), ('model', random_forest_classifier)])\n",
    "        scores = cross_validate(\n",
    "            estimator = pipeline,\n",
    "            X = X,\n",
    "            y = y,\n",
    "            cv=cv,\n",
    "            scoring=[\"f1\", \"recall\", \"precision\"]\n",
    "        )\n",
    "        mean_f1 = scores['test_f1'].mean()\n",
    "        mean_recall = scores['test_recall'].mean()\n",
    "        mean_precision = scores['test_precision'].mean()\n",
    "        \n",
    "        print(f\"n_estimators={n_trees}\")\n",
    "        print(f\"Mean F1 Score: {mean_f1:.3f}\")\n",
    "        print(f\"Mean Recall: {mean_recall:.3f}\")\n",
    "        print(f\"Mean Precision: {mean_precision:.3f}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "execute_random_forest_with_different_estimators_and_cv();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=1\n",
      "Mean F1 Score: 0.717\n",
      "Mean Recall: 0.788\n",
      "Mean Precision: 0.658\n",
      "------------------------------\n",
      "n_estimators=2\n",
      "Mean F1 Score: 0.703\n",
      "Mean Recall: 0.674\n",
      "Mean Precision: 0.742\n",
      "------------------------------\n",
      "n_estimators=5\n",
      "Mean F1 Score: 0.755\n",
      "Mean Recall: 0.846\n",
      "Mean Precision: 0.682\n",
      "------------------------------\n",
      "n_estimators=10\n",
      "Mean F1 Score: 0.761\n",
      "Mean Recall: 0.835\n",
      "Mean Precision: 0.699\n",
      "------------------------------\n",
      "n_estimators=20\n",
      "Mean F1 Score: 0.772\n",
      "Mean Recall: 0.877\n",
      "Mean Precision: 0.690\n",
      "------------------------------\n",
      "n_estimators=50\n",
      "Mean F1 Score: 0.776\n",
      "Mean Recall: 0.905\n",
      "Mean Precision: 0.680\n",
      "------------------------------\n",
      "n_estimators=100\n",
      "Mean F1 Score: 0.779\n",
      "Mean Recall: 0.917\n",
      "Mean Precision: 0.677\n",
      "------------------------------\n",
      "n_estimators=200\n",
      "Mean F1 Score: 0.779\n",
      "Mean Recall: 0.922\n",
      "Mean Precision: 0.675\n",
      "------------------------------\n",
      "\n",
      "130.08190751075745 seconds\n"
     ]
    }
   ],
   "source": [
    "@compute_executions_time\n",
    "def execute_extra_tree_with_different_estimators_and_cv(cv=10):\n",
    "    for n_trees in [1, 2, 5, 10, 20, 50, 100, 200]:\n",
    "        extra_trees = ExtraTreesClassifier(\n",
    "            n_estimators = n_trees,\n",
    "            max_features=0.35\n",
    "        )\n",
    "        pipeline = Pipeline(steps=[('undersampler', undersampler), ('model', extra_trees)])\n",
    "        scores = cross_validate(\n",
    "            estimator = pipeline,\n",
    "            X = X,\n",
    "            y = y,\n",
    "            cv=cv,\n",
    "            scoring=[\"f1\", \"recall\", \"precision\"]\n",
    "        )\n",
    "        mean_f1 = scores['test_f1'].mean()\n",
    "        mean_recall = scores['test_recall'].mean()\n",
    "        mean_precision = scores['test_precision'].mean()\n",
    "        \n",
    "        print(f\"n_estimators={n_trees}\")\n",
    "        print(f\"Mean F1 Score: {mean_f1:.3f}\")\n",
    "        print(f\"Mean Recall: {mean_recall:.3f}\")\n",
    "        print(f\"Mean Precision: {mean_precision:.3f}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "execute_extra_tree_with_different_estimators_and_cv();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=1\n",
      "Mean F1 Score: 0.702\n",
      "Mean Recall: 0.958\n",
      "Mean Precision: 0.554\n",
      "------------------------------\n",
      "n_estimators=2\n",
      "Mean F1 Score: 0.702\n",
      "Mean Recall: 0.958\n",
      "Mean Precision: 0.554\n",
      "------------------------------\n",
      "n_estimators=5\n",
      "Mean F1 Score: 0.706\n",
      "Mean Recall: 0.940\n",
      "Mean Precision: 0.565\n",
      "------------------------------\n",
      "n_estimators=10\n",
      "Mean F1 Score: 0.709\n",
      "Mean Recall: 0.900\n",
      "Mean Precision: 0.585\n",
      "------------------------------\n",
      "n_estimators=20\n",
      "Mean F1 Score: 0.712\n",
      "Mean Recall: 0.880\n",
      "Mean Precision: 0.598\n",
      "------------------------------\n",
      "n_estimators=50\n",
      "Mean F1 Score: 0.717\n",
      "Mean Recall: 0.874\n",
      "Mean Precision: 0.607\n",
      "------------------------------\n",
      "n_estimators=100\n",
      "Mean F1 Score: 0.717\n",
      "Mean Recall: 0.871\n",
      "Mean Precision: 0.609\n",
      "------------------------------\n",
      "n_estimators=200\n",
      "Mean F1 Score: 0.717\n",
      "Mean Recall: 0.870\n",
      "Mean Precision: 0.610\n",
      "------------------------------\n",
      "\n",
      "88.95444440841675 seconds\n"
     ]
    }
   ],
   "source": [
    "@compute_executions_time\n",
    "def execute_ada_boost_classifier_for_different_classifiers(cv=10):\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    for n_estimators in [1, 2, 5, 10, 20, 50, 100, 200]:\n",
    "        ada_boos_classifier = AdaBoostClassifier(\n",
    "            n_estimators=n_estimators\n",
    "        )\n",
    "        pipeline = Pipeline(steps=[('undersampler', undersampler), ('model', ada_boos_classifier)])\n",
    "        scores = cross_validate(\n",
    "            pipeline,\n",
    "            X,\n",
    "            y,\n",
    "            cv = cv,\n",
    "            scoring=[\"f1\", \"recall\", \"precision\"]\n",
    "        )\n",
    "        mean_f1 = scores['test_f1'].mean()\n",
    "        mean_recall = scores['test_recall'].mean()\n",
    "        mean_precision = scores['test_precision'].mean()\n",
    "        \n",
    "        print(f\"n_estimators={n_estimators}\")\n",
    "        print(f\"Mean F1 Score: {mean_f1:.3f}\")\n",
    "        print(f\"Mean Recall: {mean_recall:.3f}\")\n",
    "        print(f\"Mean Precision: {mean_precision:.3f}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "execute_ada_boost_classifier_for_different_classifiers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=1\n",
      "Mean F1 Score: 0.704\n",
      "Mean Recall: 0.949\n",
      "Mean Precision: 0.559\n",
      "------------------------------\n",
      "n_estimators=2\n",
      "Mean F1 Score: 0.704\n",
      "Mean Recall: 0.950\n",
      "Mean Precision: 0.559\n",
      "------------------------------\n",
      "n_estimators=5\n",
      "Mean F1 Score: 0.705\n",
      "Mean Recall: 0.953\n",
      "Mean Precision: 0.559\n",
      "------------------------------\n",
      "n_estimators=10\n",
      "Mean F1 Score: 0.705\n",
      "Mean Recall: 0.954\n",
      "Mean Precision: 0.559\n",
      "------------------------------\n",
      "n_estimators=20\n",
      "Mean F1 Score: 0.705\n",
      "Mean Recall: 0.951\n",
      "Mean Precision: 0.560\n",
      "------------------------------\n",
      "n_estimators=50\n",
      "Mean F1 Score: 0.715\n",
      "Mean Recall: 0.919\n",
      "Mean Precision: 0.585\n",
      "------------------------------\n",
      "n_estimators=100\n",
      "Mean F1 Score: 0.718\n",
      "Mean Recall: 0.897\n",
      "Mean Precision: 0.599\n",
      "------------------------------\n",
      "n_estimators=200\n",
      "Mean F1 Score: 0.720\n",
      "Mean Recall: 0.884\n",
      "Mean Precision: 0.608\n",
      "------------------------------\n",
      "\n",
      "207.97303462028503 seconds\n"
     ]
    }
   ],
   "source": [
    "@compute_executions_time\n",
    "def execute_gradient_boosting_classifier_for_different_classifiers(cv=10):\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    for n_estimators in [1, 2, 5, 10, 20, 50, 100, 200]:\n",
    "        gradient = GradientBoostingClassifier(\n",
    "            n_estimators=n_estimators\n",
    "        )\n",
    "        pipeline = Pipeline(steps=[('undersampler', undersampler), ('model', gradient)])\n",
    "        scores = cross_validate(\n",
    "            pipeline,\n",
    "            X,\n",
    "            y,\n",
    "            cv = cv,\n",
    "            scoring=[\"f1\", \"recall\", \"precision\"]\n",
    "        )\n",
    "        mean_f1 = scores['test_f1'].mean()\n",
    "        mean_recall = scores['test_recall'].mean()\n",
    "        mean_precision = scores['test_precision'].mean()\n",
    "        \n",
    "        print(f\"n_estimators={n_estimators}\")\n",
    "        print(f\"Mean F1 Score: {mean_f1:.3f}\")\n",
    "        print(f\"Mean Recall: {mean_recall:.3f}\")\n",
    "        print(f\"Mean Precision: {mean_precision:.3f}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "execute_gradient_boosting_classifier_for_different_classifiers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
